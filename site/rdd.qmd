---
title: "Regression Discontinuity"
subtitle: "ECON526"
author:
  - name: Paul Schrimpf
    email: paul.schrimpf@ubc.ca
    affiliations: University of British Columbia
format:
    revealjs:
        min-scale: 0.1
        smaller: true
        toc: true
        toc-depth: 1
        progress: true
        chalkboard:
          theme: whiteboard
          boardmarker-width: 2
          chalk-width: 2
          chalk-effect: 0.0
        title-slide-attributes:
          data-background-image: "kokaneesunset.jpg"
          data-background-size: contain
execute:
  cache: true
  echo: true
bibliography: 526.bib
---


# Introduction

$$
\def\indep{\perp\!\!\!\perp} % \def\idp{\perp\kern-5pt\perp}
\def\Er{\mathrm{E}}
\def\var{\mathrm{Var}}
\def\cov{\mathrm{Cov}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\def\inprob{{\,{\buildrel p \over \rightarrow}\,}}
\def\indist{\,{\buildrel d \over \rightarrow}\,}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

# Sharp Discontinuity

## Regression Discontinuity

::: {.columns}

::: {.column width=50%}

- Treatment $D_i \in \{0,1\}$
- Potential outcomes $Y_i(d)$
- Running variable $R_i$, treatment assignment discontinuous in $r$ at cutoff $c$
  - Sharp: $P(D|R)$ jumps from 0 to 1
  - Fuzzy: $P(D|R)$ jumps

:::

::: {.column width=50%}

```{python}
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('tableau-colorblind10')

def pdr(r):
    if (r < 0):
        return 0 #0.2/(1+np.exp(-r))
    else:
        return 1 # 0.8/(1+0.5*np.exp(-r))

def plotp(ax, pdr):
    r = np.linspace(-2,2,500)
    ax.plot(r, list(map(pdr,r)), color='C0')
    ax.set_xlabel('R')
    ax.set_ylabel('Pr(D=1|R)')
    return(ax)
fig, ax = plt.subplots(figsize=(5,5))
ax = plotp(ax,pdr)
```

:::

:::

## Running Variables and Discontinuities

- Usually come from institutional rules
- Treatment / program eligibility changes discretely with $R$
- Common running variables:
  - Geographic location
  - Income, wealth
  - Test scores
  - Votes
  - Age

## Continuous Potential Outcomes

::: {.columns}

::: {.column width="60%"}
- Assume continuity:
$$\Er[Y(1)|R=r]$$
and
$$\Er[Y(0)|R=r]$$
are continuous in $r$
- Observed
$$
\begin{align*}
\Er[Y|R] = & \Pr(D=1|R)\Er[Y(1)|R,D=1] + \\ & + \Pr(D=0|R)\Er[Y(0)|R,D=0]
\end{align*}
$$
- Idea: size of discontinuity in $\Er[Y|R]$ is related to a treatment effect
:::

::: {.column width="40%"}
```{python}
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('tableau-colorblind10')

def Ey(d,r) :
    if d < 0.1 :
        return 0.5*(r+0.2)**3 + 0.1*(r+0.2)**2
    else :
        return 2 + 0.3*r**3 + r

fig, ax = plt.subplots(2,1, figsize=(4,7))
ax[0] = plotp(ax[0],pdr)

def plotey(ax,Ey,pdr) :
    r = np.linspace(-2,-0.01,100)
    ax.plot(r, list(map(lambda r: Ey(0,r),r)), color='C0', label="E[Y(0)|R]")
    r = np.linspace(2,0.01,100)
    ax.plot(r, list(map(lambda r: Ey(0,r),r)), color='C0', linestyle=":")
    r = np.linspace(-2,-0.01,100)
    ax.plot(r, list(map(lambda r: Ey(1,r),r)), color='C1', linestyle=":")
    r = np.linspace(2,0.01,100)
    ax.plot(r, list(map(lambda r: Ey(1,r),r)), color='C1', label="E[Y(1)|R]")
    r = np.linspace(-2,2,200)
    ax.plot(r,list(map(lambda r: pdr(r)*Ey(1,r) + (1-pdr(r))*Ey(0,r),r)), color='C2', label="E[Y|R]", linestyle="--", alpha=0.8)
    ax.legend()
    ax.set_xlabel('R')
    ax.set_ylabel('E[Y(d)|R]')
    return(ax)

ax[1] = plotey(ax[1],Ey,pdr)

```
:::

:::

## Identification

- Size of disconuity in $\Er[Y|R]$
$$
\begin{align*}
  \lim_{r \downarrow c} \Er[Y|R=r] - \lim_{r \uparrow c} \Er[Y|R=r] & = \lim_{r \downarrow c} \Er[Y(1)|R=r] - \lim_{r \uparrow c} \Er[Y(0),r)|R=r] \\
  & = \Er[Y(1) - Y(0) | R=c]
\end{align*}
$$
- Identifies ATE conditional on being at the cutoff
- Assuming:
   1. Sharp discontinuity $P(D|R=r) = \begin{cases} 0 & \; r<c \\ 1 & r \geq c \end{cases}$
   2. Continuity of $\Er[Y(1)|R=r]$ and $\Er[Y(0)|R=r]$

## Data

```{python}
#| code-fold: true
n = 1_000
sig = 2
fig,ax=plt.subplots()
r = np.random.rand(n)*4-2
y = np.vectorize(lambda r: Ey(np.sign(r)*2+1,r) + np.random.randn()*sig)(r)
ax =plotey(ax,Ey,pdr)
ax.scatter(r,y,color='C2', alpha=0.5,s=1)
plt.show()
```

## Estimation

- Fit regression to left and right of discontinuity using only observations near the cutoffs

```{python}
#| output-location: slide
import pandas as pd
import statsmodels.formula.api as smf
from statsmodels.iolib.summary2 import summary_col

df = pd.DataFrame({'y':y,'r':r})

def rdd(df, h, c=0, R='r', Y='y') :
    wdf = df.loc[np.abs(df[R]-c)<=h]
    m=smf.ols(f'{Y} ~ I({R}-c)*I({R}>c)',wdf).fit(cov_type="HC3")
    return(m)

bandwidths=[.25, 0.5, 1., 2.]
models = [rdd(df,h) for h in bandwidths]
summary_col(models, model_names=[f"h={h:.2}" for h in bandwidths])
```

## Plotting Estimates

```{python}
#| output-location: slide
def plotrdd(ax,df, h, c=0, R='r',Y='y', Ey=Ey) :
    df = df.sort_values(R)
    m = rdd(df,h,c,R,Y)
    df.plot.scatter(x=R,y=Y,ax=ax,color="C2",alpha=0.5,s=1)
    df.assign(predictions=m.fittedvalues).plot(x=R, y="predictions",label='Estimate', ax=ax, color="C3")
    df.assign(Ey0=df[R].apply(lambda r: Ey(0,r))).plot(x=R,y='Ey0',label="E[Y(0)|R]",ax=ax,color="C0",linestyle="--")
    df.assign(Ey1=df[R].apply(lambda r: Ey(1,r))).plot(x=R,y='Ey1',label="E[Y(1)|R]",ax=ax,color="C1",linestyle="--")
    #ax.get_legend().remove()
    return(ax)

fig,ax = plt.subplots(2,2, figsize=(10,5))
for (i,h) in enumerate(bandwidths) :
    plotrdd(ax.flat[i],df,h)
    ax.flat[i].set_title(f"h={h:.2}")
```

## Questions

- Is there a better way to visualize?
- How to choose `h`?
- Are these standard errors correct?
- Are there any falsification or other checks to do?

## Binned Scatter Plot

::: {.columns}

::: {.column width=60%}

- Divide range of $R$ into bins, plot mean within each bin
- Many papers just show binned means, but better to show uncertainty / variability in data too
  - Two good options in `rdrobust` package:
     1. `binselect='es'` or `qs'` and plot confidence intervals
     2. `binselect='esmv'` or `'qsmv'`
- See @cattaneo2019 section 3 and @binscatter2024

:::

::: {.column width=40%}

```{python}
#| code-fold : true
import rdrobust
rdp = rdrobust.rdplot(df.y,df.r,c=0,hide=True, binselect='es')
fg,ax=plt.subplots(figsize=(4,6))
pltdf = rdp.vars_bins
pltdf.plot.scatter(x='rdplot_mean_bin',y='rdplot_mean_y', color='black',ax=ax,label='IMSE optimal bins')
ax.set_xlabel('R')
ax.set_ylabel('Y')
ax.errorbar(x=pltdf.rdplot_mean_bin,y=pltdf.rdplot_mean_y,
             yerr=[pltdf.rdplot_mean_y-pltdf.rdplot_ci_l,pltdf.rdplot_ci_r - pltdf.rdplot_mean_y],
             ls='none',color="black")
rdp = rdrobust.rdplot(df.y,df.r,c=0,hide=True, binselect='esmv')
pltdf = rdp.vars_bins
pltdf.plot.scatter(x='rdplot_mean_bin',y='rdplot_mean_y', color='C4',ax=ax,label='Variance mimicking bins')
df.plot.scatter(x='r',y='y',color='C2',s=1,alpha=0.5,label=None,ax=ax)
ax.legend()
plt.show()
```
:::

:::

## Bandwidth Selection

- Bandwith, `h`, has bias variance tradeoff
- Larger `h` $\Rightarrow$ lower variance, higher bias
- Smaller `h` $\Rightarrow$ higher variance, lower bias
- Optimal `h` balances bias and variance
- Optimal `h` will decrease with sample size

## Bandwidth Selection

```{python}
rd = rdrobust.rdrobust(df.y,df.r, kernel="uniform",  bwselect="msetwo")
rd
```

## Confidence Intervals

- Optimal `h` has $\mathrm{Bias}^2 = \var$
- Need to correct for bias for confidence intervals to be correct
- Use "Robust" interval reported by `rdrobust`
- See section 4.3 of @cattaneo2019

## Kernel Weighting

::: {.columns}

::: {.column width=40%}

- Instead of treating all observations within bandwith as equally important for estimating discontinuity, we might want to weight observations closer to discontinuity more
- "triangular" kernel is best

:::

::: {.column width=60%}

```{python}
rd = rdrobust.rdrobust(df.y,df.r, kernel="triangular",  bwselect="msetwo")
rd
```
:::

:::


## Manipulation of Running Variable

- If units can change $R_i$, they might do in way to change treatment status
- Manipulation of $R_i$ makes continuity of $\Er[Y(d)|R]$ less plausible
- Check for bunching in density of $R_i$ near cutoff

## Manipulation of Running Variable

```{python}
fig,ax=plt.subplots()
ax.hist(x=df.r[df.r<=0],bins=20,color="C0")
ax.hist(x=df.r[df.r>0],bins=20,color="C1")
ax.set_xlim(-2,2)
ax.axvline(0,color="black")
plt.show()
```

## Placebo Tests

::: {.columns}

::: {.column width=40%}

- If have data on outcomes not affected by treatment (e.g. if predetermined) can check that RD estimate for them is 0

:::

::: {.column width=60%}

```{python}
df.x = np.sin(2*df.r)*np.exp(0.5*df.r) + np.random.randn(df.shape[0])
rdx = rdrobust.rdrobust(df.x,df.r, kernel="triangular",  bwselect="msetwo")
rdx
```
:::
:::

## Placebo Tests

```{python}
rdpx = rdrobust.rdplot(df.x,df.r,c=0,hide=False, binselect='es',x_label="R",y_label="x",ci=95)
```

# Fuzzy Discontinuity

## Fuzzy Discontinuity

::: {.columns}

::: {.column width=50%}

- $P(D|R)$ discontinuous at $c$
- imperfect compliance
- Idea: use discontinuity as instrument for treatment

:::

::: {.column width=50%}
```{python}
#| code-fold: true
def pdr(r):
    if (r < 0):
        return 0.2/(1+np.exp(-r))
    else:
        return 0.8/(1+0.5*np.exp(-r))

fig, ax = plt.subplots(figsize=(5,5))
ax = plotp(ax,pdr)
```
:::

:::

## Merit Based Financial Aid for Low-Income Students

- @lrs2020 (also used as example in @cattaneo2024)
- Full undergraduate tuition available if
  1. Standardized test $\geq$ 91%-tile
  2. Wealth index $\leq$ cutoff(region)
- Two discontinuities
- Outcome $=$ postsecondary enrollment
- Focus on wealth cutoff


## Merit Based Financial Aid for Low-Income Students

```{python}
lrs_all = pd.read_stata('data/data_RD.dta')
lrs = lrs_all.loc[lrs_all.eligible_saber11==1] #
lrs.columns
```

## First Stage: Effect on Receiving Tuition Subsidy


```{python}
#| output-location: slide
df=lrs[["running_sisben","beneficiary_spp","icfes_score_20142","spadies_any","spadies_hq"]].dropna()
rdrobust.rdplot(df.beneficiary_spp, df.running_sisben, binselect='esmv', x_label="Wealth Index",y_label="P(D|wealth)")
```

## First Stage: Effect on Receiving Tuition Subsidy

```{python}
fs = rdrobust.rdrobust(df.beneficiary_spp, df.running_sisben, kernel="triangular",  bwselect="mserd")
fs
```

## Reduced Form: Effect on Postsecondary Enrollment

```{python}
#| output-location: slide
rdrobust.rdplot(df.spadies_any, df.running_sisben, binselect='esmv', x_label="Wealth Index",y_label="P(postsecondary|wealth)")
```

## Reduced Form: Effect on Postsecondary Enrollment

```{python}
rf = rdrobust.rdrobust(df.spadies_any, df.running_sisben, kernel="triangular",  bwselect="mserd")
rf
```

## Questions

- How to get an IV estimate?
- What causal interpretation can we give an IV estimate?


## Potential Outcomes

- "Assigned treatment $A_i = 1\{R_i > c\}$
- Potential treatments $D_i(A_i) \in \{0,1\}$
- Potential outcomes $Y_i(a, d)$
- Observed outcome $Y_i(A_i,D_i(A_i))$
- Exclusion restriction: $R_i$ does not affect treatment or outcome, except through $A_i$

## LATE

- Assume monotonicity: $D_i(1) \geq D_i(0)$
$$
\frac{\text{reduced form}}{\text{first stage}} \inprob \Er[Y(1) - Y(0) | wealth=c_w, test>c_t, D_i(1)>D_i(0)]
$$
- Optimal bandwidth choice different
- Use `rdrobust` for bandwidth selection and confidence intervals

## LATE

```{python}
late = rdrobust.rdrobust(df.spadies_any, df.running_sisben, fuzzy=df.beneficiary_spp)
late
```


## Sources and Further Reading

- @facure2022 [chapter 16](https://matheusfacure.github.io/python-causality-handbook/16-Regression-Discontinuity-Design.html)
- @causalml2024 chapter 17
- @cattaneo2019 and @cattaneo2024
- https://rdpackages.github.io/

## References
