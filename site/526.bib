@misc{borusyak2018,
  title={Revisiting event study designs},
  author={Borusyak, Kirill and Jaravel, Xavier},
  year={2018},
  url={https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf}
}

@book{facure2022,
 title={Causal Inference for the Brave and True},
 url={https://matheusfacure.github.io/python-causality-handbook/landing-page.html},
 author={Facure, Matheus},
 year={2022}
}

@book{hk2021,
 title={The Effect: An Introduction to Research Design and Causality},
 url={https://theeffectbook.net/},
 author={Huntington-Klein, Nick},
 year={2021},
 publisher={CRC Press}
}

@article{abadie2008,
author = {Abadie, Alberto and Imbens, Guido W.},
title = {On the Failure of the Bootstrap for Matching Estimators},
journal = {Econometrica},
volume = {76},
number = {6},
pages = {1537-1557},
keywords = {Average treatment effects, bootstrap, matching},
doi = {https://doi.org/10.3982/ECTA6474},
url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA6474},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA6474},
abstract = {Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romano (1994).},
year = {2008}
}


@misc{athey2019,
Author = {Susan Athey and Stefan Wager},
Title = {Estimating Treatment Effects with Causal Forests: An Application},
Year = {2019},
Eprint = {arXiv:1902.07409},
}

﻿@article{yeager2019,
author={Yeager, David S.
and Hanselman, Paul
and Walton, Gregory M.
and Murray, Jared S.
and Crosnoe, Robert
and Muller, Chandra
and Tipton, Elizabeth
and Schneider, Barbara
and Hulleman, Chris S.
and Hinojosa, Cintia P.
and Paunesku, David
and Romero, Carissa
and Flint, Kate
and Roberts, Alice
and Trott, Jill
and Iachan, Ronaldo
and Buontempo, Jenny
and Yang, Sophia Man
and Carvalho, Carlos M.
and Hahn, P. Richard
and Gopalan, Maithreyi
and Mhatre, Pratik
and Ferguson, Ronald
and Duckworth, Angela L.
and Dweck, Carol S.},
title={A national experiment reveals where a growth mindset improves achievement},
journal={Nature},
year={2019},
month={Sep},
day={01},
volume={573},
number={7774},
pages={364-369},
abstract={A global priority for the behavioural sciences is to develop cost-effective, scalable interventions that could improve the academic outcomes of adolescents at a population level, but no such interventions have so far been evaluated in a population-generalizable sample. Here we show that a short (less than one hour), online growth mindset intervention---which teaches that intellectual abilities can be developed---improved grades among lower-achieving students and increased overall enrolment to advanced mathematics courses in a nationally representative sample of students in secondary education in the United States. Notably, the study identified school contexts that sustained the effects of the growth mindset intervention: the intervention changed grades when peer norms aligned with the messages of the intervention. Confidence in the conclusions of this study comes from independent data collection and processing, pre-registration of analyses, and corroboration of results by a blinded Bayesian analysis.},
issn={1476-4687},
doi={10.1038/s41586-019-1466-y},
url={https://doi.org/10.1038/s41586-019-1466-y}
}

@article{dd2022,
    author = {de Chaisemartin, Clément and D’Haultfœuille, Xavier},
    title = "{Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey}",
    journal = {The Econometrics Journal},
    volume = {26},
    number = {3},
    pages = {C1-C30},
    year = {2022},
    month = {06},
    abstract = "{Linear regressions with period and group fixed effects are widely used to estimate policie’s effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been shown that those regressions may produce misleading estimates if the policy’s effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue and that proposes alternative estimators robust to heterogeneous effects. We use those alternative estimators to revisit Wolfers (2006a).}",
    issn = {1368-4221},
    doi = {10.1093/ectj/utac017},
    url = {https://doi.org/10.1093/ectj/utac017},
    eprint = {https://academic.oup.com/ectj/article-pdf/26/3/C1/51707976/utac017.pdf},
}

@article{roth2023,
title = {What’s trending in difference-in-differences? A synthesis of the recent econometrics literature},
journal = {Journal of Econometrics},
volume = {235},
number = {2},
pages = {2218-2244},
year = {2023},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2023.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0304407623001318},
author = {Jonathan Roth and Pedro H.C. Sant’Anna and Alyssa Bilinski and John Poe},
keywords = {Difference-in-differences, Causal Inference, Staggered Treatment timing, Sensitivity Analysis, Clustering, Parallel trends, Treatment Effect Heterogeneity},
abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of “canonical” assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on (i) multiple periods and variation in treatment timing, (ii) potential violations of parallel trends, or (iii) alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.}
}



@article{chen2019,
Author = {Chen, Shuowen and Chernozhukov, Victor and Fernández-Val, Iván},
Title = {Mastering Panel Metrics: Causal Impact of Democracy on Growth},
Journal = {AEA Papers and Proceedings},
Volume = {109},
Year = {2019},
Month = {May},
Pages = {77-82},
DOI = {10.1257/pandp.20191071},
URL = {https://www.aeaweb.org/articles?id=10.1257/pandp.20191071}}


@article{abadie2023,
    author = {Abadie, Alberto and Athey, Susan and Imbens, Guido W and Wooldridge, Jeffrey M},
    title = {When Should You Adjust Standard Errors for Clustering?},
    journal = {The Quarterly Journal of Economics},
    volume = {138},
    number = {1},
    pages = {1-35},
    year = {2023},
    month = {2},
    abstract = {Clustered standard errors, with clusters defined by factors such as geography, are widespread in empirical research in economics and many other disciplines. Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster-level components. However, the standard econometric framework for clustering leaves important questions unanswered: (i) Why do we adjust standard errors for clustering in some ways but not others, for example, by state but not by gender, and in observational studies but not in completely randomized experiments? (ii) Is the clustered variance estimator valid if we observe a large fraction of the clusters in the population? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these and other questions using a novel framework for clustered inference on average treatment effects. In addition to the common sampling component, the new framework incorporates a design component that accounts for the variability induced on the estimator by the treatment assignment mechanism. We show that, when the number of clusters in the sample is a nonnegligible fraction of the number of clusters in the population, conventional clustered standard errors can be severely inflated, and propose new variance estimators that correct for this bias.},
    issn = {0033-5533},
    doi = {10.1093/qje/qjac038},
    url = {https://doi.org/10.1093/qje/qjac038},
    eprint = {https://academic.oup.com/qje/article-pdf/138/1/1/47915437/qjac038.pdf},
}


@article{mackinon2023,
title = {Cluster-robust inference: A guide to empirical practice},
journal = {Journal of Econometrics},
volume = {232},
number = {2},
pages = {272-299},
year = {2023},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304407622000781},
author = {James G. MacKinnon and Morten Ørregaard Nielsen and Matthew D. Webb},
keywords = {Clustered data, Cluster jackknife, Cluster-robust variance estimator (CRVE), Robust inference, Wild cluster bootstrap},
abstract = {Methods for cluster-robust inference are routinely used in economics and many other disciplines. However, it is only recently that theoretical foundations for the use of these methods in many empirically relevant situations have been developed. In this paper, we use these theoretical results to provide a guide to empirical practice. We do not attempt to present a comprehensive survey of the (very large) literature. Instead, we bridge theory and practice by providing a thorough guide on what to do and why, based on recently available econometric theory and simulation evidence. To practice what we preach, we include an empirical analysis of the effects of the minimum wage on labor supply of teenagers using individual data.}}

@article{sz2020,
title = {Doubly robust difference-in-differences estimators},
journal = {Journal of Econometrics},
volume = {219},
number = {1},
pages = {101-122},
year = {2020},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2020.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304407620301901},
author = {Pedro H.C. Sant’Anna and Jun Zhao},
keywords = {Causal inference, Difference-in-differences, Natural experiments, Panel data, Repeated cross-section data, Semiparametric efficiency},
abstract = {This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying particular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available.}
}